[
  {
    "question": "Describe the bias-variance tradeoff in machine learning.",
    "ideal_answer": "The bias-variance tradeoff is a fundamental concept where bias is the error from erroneous assumptions in the learning algorithm (underfitting), and variance is the error from sensitivity to small fluctuations in the training set (overfitting). A simple model has high bias and low variance, while a complex model has low bias and high variance. The goal is to find a balance that minimizes total error.",
    "tags": [
      "machine-learning",
      "foundations"
    ],
    "difficulty": "beginner"
  },
  {
    "question": "Explain the difference between L1 and L2 regularization and their effects on model weights.",
    "ideal_answer": "L1 regularization (Lasso) adds a penalty equal to the absolute value of the magnitude of coefficients, which can lead to sparse models by shrinking some coefficients to zero. L2 regularization (Ridge) adds a penalty equal to the square of the magnitude of coefficients, which shrinks coefficients towards zero but doesn't typically make them exactly zero. L1 is useful for feature selection.",
    "tags": [
      "machine-learning",
      "regularization"
    ],
    "difficulty": "intermediate"
  },
  {
    "question": "How would you handle an imbalanced dataset in a classification problem?",
    "ideal_answer": "Several techniques can be used. Resampling methods like oversampling the minority class (e.g., SMOTE) or undersampling the majority class. Using different evaluation metrics like F1-score, Precision-Recall AUC instead of accuracy. Cost-sensitive learning, where the model is penalized more for misclassifying the minority class. Or using algorithms that inherently handle imbalance.",
    "tags": [
      "machine-learning",
      "classification",
      "data-preprocessing"
    ],
    "difficulty": "intermediate"
  },
  {
    "question": "What are transformer models and what is the role of the self-attention mechanism?",
    "ideal_answer": "Transformers are a type of neural network architecture primarily used for NLP tasks. The self-attention mechanism allows the model to weigh the importance of different words in the input sequence when processing a particular word, capturing contextual relationships regardless of their distance from each other.",
    "tags": [
      "deep-learning",
      "nlp",
      "transformers"
    ],
    "difficulty": "advanced"
  },
  {
    "question": "What is the difference between a normal convolutional layer and a depthwise separable convolution?",
    "ideal_answer": "A standard convolution performs channel-wise and spatial-wise computation in one step. A depthwise separable convolution splits this into two steps: a depthwise convolution that performs spatial convolution independently for each input channel, followed by a pointwise convolution (1x1 convolution) that combines the outputs of the depthwise convolution. This significantly reduces the number of parameters and computational cost.",
    "tags": [
      "deep-learning",
      "computer-vision",
      "cnn"
    ],
    "difficulty": "advanced"
  }
]