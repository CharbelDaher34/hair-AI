services:
  db:
    image: postgres:16
    container_name: local_pgdb
    restart: on-failure
    ports:
      - "5437:5432"
    environment:
      POSTGRES_USER: charbel
      POSTGRES_PASSWORD: charbel
      POSTGRES_DB: matching_db
    volumes:
      - local_pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U charbel -d matching_db"]
      interval: 5s
      timeout: 3s
      retries: 15
  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin4_container
    restart: on-failure
    ports:
      - "8888:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: charbeldaher34@gmail.com
      PGADMIN_DEFAULT_PASSWORD: charbel
    volumes:
      - pgadmin-data:/var/lib/pgadmin
  backend:
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: backend
    restart: on-failure
    ports: 
      - "8017:8017"
    depends_on:
      db:
        condition: service_healthy
      ai:
        condition: service_started
    # Load environment variables from backend .env file
    env_file:
      - ./backend/app/.env
    volumes:
      - ./backend/app/static:/static
      - ./backend/app/resumes:/resumes
  ai:
    build:
      context: ./ai
      dockerfile: Dockerfile
    container_name: ai
    restart: on-failure
    ports: 
      - "8015:8011"
    # Load environment variables from ai .env file
    env_file:
      - ./ai/app/.env
    volumes:
      - ./ai/app/static:/app/static
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: frontend
    restart: on-failure
    ports: 
      - "8080:8080"
    depends_on:
      - backend
      - db
    environment:
      - PROXY_PASS_URL=backend:8017
      - VITE_DOCKER=true
      - VITE_API_URL=http://84.16.230.94:8017/api/v1
    volumes:
      - ./frontend/public:/app/public

  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    ports:
      - "11439:11439"  # Correct internal and external port
    environment:
      - OLLAMA_HOST=0.0.0.0:11439  # Ensure Ollama binds to correct port
    volumes:
      - ./ollama-data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    entrypoint: ["sh", "-c", "ollama serve & sleep 5 && ollama run qwen3:8b && tail -f /dev/null"]
    restart: unless-stopped


volumes:
  local_pgdata:
  pgadmin-data:
  ollama_data:
    name: ollama_data